{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apache Airflow is an open-source platform to Author, Schedule and Monitor workflows. \n",
    "#It was created at Airbnb and currently is a part of Apache Software Foundation. \n",
    "#Airflow helps you to create workflows using Python programming language and these workflows can be scheduled \n",
    "#and monitored easily with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAG - Directed Acyclic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define uma sequência de tarefas, com dependências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow run <dag_id> <task_id> <start_date>\n",
    "\n",
    "airflow run example-etl download-file 2020-01-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "#Opcionais, mas é sempre bom usar\n",
    "default_arguments = {\n",
    "    'owner':'preco',\n",
    "    'email':'rodrigopreco@gmail.com',\n",
    "    'start_date':datetime(2021,4,21),\n",
    "    'retries':2\n",
    "}\n",
    "\n",
    "etl_dag = DAG( 'etl_workflow', default_args=default_arguments )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The airflow command line program contains many subcommands.\n",
    "airflow -h for descriptions.\n",
    "\n",
    "#Many are related to DAGs.\n",
    "airflow list_dags #to show all recognized DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BashOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BashOperator\n",
    "#Executa um comando bash\n",
    "#Roda o comando em um diretorio temporario\n",
    "#Pode especificar variaveis de ambiente\n",
    "BashOperator(\n",
    "            task_id='bash_example',\n",
    "            bash_command='echo \"Example!\"',\n",
    "            dag=ml_dag)\n",
    "\n",
    "BashOperator(task_id='bash_script_example',\n",
    "             bash_command='runcleanup.sh',\n",
    "             dag=ml_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.bash_operator import BashOperator\n",
    "example_task = BashOperator(task_id='bash_ex',\n",
    "                            bash_command='echo 1',\n",
    "                            dag=dag)\n",
    "bash_task = BashOperator(task_id='clean_addresses',\n",
    "                            bash_command='cat addresses.txt | awk \"NF==10\" > cleaned.txt',\n",
    "                            dag=dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Executes a Python function / callable\n",
    "#Operates similarly to the BashOperator, with more options\n",
    "#Can pass in arguments to the Python code\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "def printme():\n",
    "    return print(\"This goes in the logs!\")\n",
    "python_task = PythonOperator(\n",
    "                            task_id='simple_print',\n",
    "                            python_callable=printme,\n",
    "                            dag=example_dag\n",
    "                            )\n",
    "\n",
    "def sleep(length_of_time):\n",
    "    return time.sleep(length_of_time)\n",
    "sleep_task = PythonOperator(\n",
    "                            task_id='sleep',\n",
    "                            python_callable=sleep,\n",
    "                            op_kwargs={'length_of_time': 5}\n",
    "                            dag=example_dag\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmailOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found in the airflow.operators library\n",
    "#Sends an email\n",
    "#Can contain typical components\n",
    "#HTML content\n",
    "#Attachments\n",
    "#Does require the Air.ow system to be con,gured with email server details\n",
    "\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "email_task = EmailOperator(\n",
    "                            task_id='email_sales_report',\n",
    "                            to='sales_manager@example.com',\n",
    "                            subject='Automated Sales Report',\n",
    "                            html_content='Attached is the latest sales report',\n",
    "                            files='latest_sales.xlsx',\n",
    "                            dag=example_dag\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instances of operators\n",
    "#Usually assigned to a variable in Python\n",
    "#Referred to by the task_id within the Air.ow tools\n",
    "\n",
    "example_task = BashOperator(task_id='bash_example',\n",
    "                            bash_command='echo \"Example!\"',\n",
    "                            dag=dag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a given order of task completion\n",
    "#Are not required for a given work.ow, but usually present in most\n",
    "#Are referred to as upstream or downstream tasks\n",
    "#In Air.ow 1.8 and later, are de,ned using the bitshi/ operators\n",
    "#>>, or the upstream operator\n",
    "#<<, or the downstream operator\n",
    "#Upstream means before\n",
    "#Downstream means after\n",
    "\n",
    "# Define the tasks\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                    bash_command='echo 1',\n",
    "                    dag=example_dag)\n",
    "\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                    bash_command='echo 2',\n",
    "                    dag=example_dag)\n",
    "\n",
    "# Set first_task to run before second_task\n",
    "task1 >> task2 # or task2 << task1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_date - The date / time to initially schedule the DAG run\n",
    "#end_date - Optional for when to stop running new DAG instances\n",
    "#max_tries - Optional atribute for how many attempts\n",
    "#schedule_interval\n",
    "    #How often to schedule the DAG\n",
    "    #Between the start_date and end_date\n",
    "    #Can be defined via cron style syntax or via built-in presets.\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2020,2,20), schedule_interval='@None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cron Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is pulled from the Unix cron format\n",
    "#0 12 * * * # Run daily at noon\n",
    "#* * 25 2 * # Run once per minute on February 25\n",
    "#0,15,30,45 * * * * # Run every 15 minutes\n",
    "\n",
    "#Preset:\n",
    "#@hourly\n",
    "#@daily\n",
    "#@weekly\n",
    "#@monthly\n",
    "#@yearly\n",
    "#cron equivalent:\n",
    "#0 * * * *\n",
    "#0 0 * * *\n",
    "#0 0 * * 0\n",
    "#0 0 1 * *\n",
    "#0 0 1 1 *\n",
    "\n",
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "#Everyday midnight\n",
    "report_dag = DAG(\n",
    "                    dag_id = 'execute_report',\n",
    "                    schedule_interval = \"0 0 * * *\",\n",
    "                    default_args=default_args\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a sensor?\n",
    "#An operator that waits for a certain condition to be true\n",
    "#Creation of a file\n",
    "#Upload of a database record\n",
    "#Certain response from a web request\n",
    "#Can define how often to check for the condition to be true\n",
    "#Are assigned to tasks\n",
    "\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "file_sensor_task = FileSensor(task_id='file_sense',\n",
    "                                filepath='salesdata.csv',\n",
    "                                poke_interval=300,\n",
    "                                dag=sales_report_dag)\n",
    "\n",
    "init_sales_cleanup >> file_sensor_task >> generate_report\n",
    "\n",
    "#Other Sensors\n",
    "#ExternalTaskSensor - wait for a task in another DAG to complete\n",
    "#HttpSensor - Request a web URL and check for content\n",
    "#SqlSensor - Runs a SQL query to check for content\n",
    "#Many others in airflow.sensors and airflow.contrib.sensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An SLA stands for Service Level Agreement\n",
    "#Within AirFLow, the amount of time a task or a DAG should require to run\n",
    "#An SLA Miss is any time the task / DAG does not meet the expected timing\n",
    "#If an SLA is missed, an email is sent out and a log is stored.\n",
    "#You can view SLA misses in the web UI.\n",
    "\n",
    "#Using the 'sla' argument on the task\n",
    "task1 = BashOperator(task_id='sla_task',\n",
    "                    bash_command='runcode.sh',\n",
    "                    sla=timedelta(seconds=30),\n",
    "                    dag=dag)\n",
    "\n",
    "#On the default_args dictionary\n",
    "default_args={\n",
    "                'sla': timedelta(minutes=20)\n",
    "                'start_date': datetime(2020,2,20)\n",
    "                }\n",
    "\n",
    "dag = DAG('sla_dag', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timedelta object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the datetime library\n",
    "#Accessed via from datetime import timedelta\n",
    "#Takes arguments of days, seconds, minutes, hours, and weeks\n",
    "#timedelta(seconds=30)\n",
    "#timedelta(weeks=2)\n",
    "#timedelta(days=4, hours=10, minutes=20, seconds=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allow substituting information during a DAG run\n",
    "#Provide added .exibility when de,ning tasks\n",
    "#Are created using the Jinja templating language\n",
    "\n",
    "templated_command=\"\"\"\n",
    "echo \"Reading {{ params.filename }}\"\n",
    "\"\"\"\n",
    "t1 = BashOperator(task_id='template_task',\n",
    "                    bash_command=templated_command,\n",
    "                    params={'filename': 'file1.txt'}\n",
    "                    dag=example_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Airflow built-in runtime variables\n",
    "#Provides assorted information about DAG runs, tasks, and even the system con,guration.\n",
    "#Examples include:\n",
    "#Execution Date: {{ ds }} # YYYY-MM-DD\n",
    "#Execution Date, no dashes: {{ ds_nodash }} # YYYYMMDD\n",
    "#Previous Execution date: {{ prev_ds }} # YYYY-MM-DD\n",
    "#Prev Execution date, no dashes: {{ prev_ds_nodash }} # YYYYMMDD\n",
    "#DAG object: {{ dag }}\n",
    "#Airflow config object: {{ conf }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In addition to others, there is also a {{ macros }} variable.\n",
    "#This is a reference to the Air.ow macros package which provides various useful objects /\n",
    "#methods for Air.ow templates.\n",
    "#{{ macros.datetime }} : The datetime.datetime object\n",
    "#{{ macros.timedelta }} : The timedelta object\n",
    "#{{ macros.uuid }} : Python's uuid object\n",
    "#{{ macros.ds_add('2020-04-15', 5) }} : Modify days from a date, this example returns\n",
    "#2020-04-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branching in Air.ow:\n",
    "#Provides conditional logic\n",
    "#Using BranchPythonOperator\n",
    "#from airflow.operators.python_operator import BranchPythonOperator\n",
    "#Takes a python_callable to return the next task id (or list of ids) to follow\n",
    "\n",
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return 'even_day_task'\n",
    "    else:\n",
    "        return 'odd_day_task'\n",
    "branch_task = BranchPythonOperator(task_id='branch_task',\n",
    "                                    dag=dag,\n",
    "                                   provide_context=True,\n",
    "                                   python_callable=branch_test)\n",
    "\n",
    "start_task >> branch_task >> even_day_task >> even_day_task2\n",
    "branch_task >> odd_day_task >> odd_day_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Completo - Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Production pipeline\n",
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from dags.process import process_data\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla':timedelta(minutes=90)\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Completo - Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviando emails condicionais\n",
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "\n",
    "no_email_task = DummyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,\n",
    "                                   dag=dag)\n",
    "\n",
    "    \n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saiba mais em : https://airflow.apache.org/docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
