{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some usefull Pyspark Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/rodri/Desktop/Spark/spark-3.0.0-preview2-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark import SparkConf , SparkContext\n",
    "from pyspark.sql import SparkSession,functions\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "config = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf=config)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Spark DataFrames - Criando DataFrames Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Preco\",35,\"VideoGames\"),\n",
    "        (\"Peter\",31,\"Pizzaiolo\"),\n",
    "        (\"Felipe\",26,\"RichStuff\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Idade\",\"Hobby\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|  Nome|Idade|     Hobby|\n",
      "+------+-----+----------+\n",
      "| Preco|   35|VideoGames|\n",
      "| Peter|   31| Pizzaiolo|\n",
      "|Felipe|   26| RichStuff|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show dataframe\n",
    "#Mostrar dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Idade: long (nullable = true)\n",
      " |-- Hobby: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nome', 'Idade', 'Hobby']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+----------+\n",
      "|summary|  Nome|             Idade|     Hobby|\n",
      "+-------+------+------------------+----------+\n",
      "|  count|     3|                 3|         3|\n",
      "|   mean|  null|30.666666666666668|      null|\n",
      "| stddev|  null| 4.509249752822894|      null|\n",
      "|    min|Felipe|                26| Pizzaiolo|\n",
      "|    max| Preco|                35|VideoGames|\n",
      "+-------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Defining - Definindo o Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StructField -> Nome do campo, tipo do campo, True (pode conter nulos)\n",
    "data_schema = [\n",
    "               StructField('Nome',StringType(),True),\n",
    "               StructField('Idade',IntegerType(),True),\n",
    "               StructField('Hobby',StringType(),True)\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Preco\",35,\"VideoGames\"),\n",
    "        (\"Peter\",31,\"Pizzaiolo\"),\n",
    "        (\"Felipe\",26,\"RichStuff\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Idade\",\"Hobby\"],final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|  Nome|Idade|     Hobby|\n",
      "+------+-----+----------+\n",
      "| Preco|   35|VideoGames|\n",
      "| Peter|   31| Pizzaiolo|\n",
      "|Felipe|   26| RichStuff|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Idade: long (nullable = true)\n",
      " |-- Hobby: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Dataframe Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Idade|\n",
      "+-----+\n",
      "|   35|\n",
      "|   31|\n",
      "|   26|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Idade').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Nome='Preco', Idade=35, Hobby='VideoGames'),\n",
       " Row(Nome='Peter', Idade=31, Hobby='Pizzaiolo'),\n",
       " Row(Nome='Felipe', Idade=26, Hobby='RichStuff')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Nome='Preco', Idade=35, Hobby='VideoGames')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Idade|  Nome|\n",
      "+-----+------+\n",
      "|   35| Preco|\n",
      "|   31| Peter|\n",
      "|   26|Felipe|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Idade','Nome']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+--------+\n",
      "|  Nome|Idade|     Hobby|Idade_x2|\n",
      "+------+-----+----------+--------+\n",
      "| Preco|   35|VideoGames|      70|\n",
      "| Peter|   31| Pizzaiolo|      62|\n",
      "|Felipe|   26| RichStuff|      52|\n",
      "+------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add columns\n",
    "#Adicionar colunas\n",
    "df.withColumn('Idade_x2',df['Idade']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+---------------+\n",
      "|  Nome|Idade|     Hobby|hardcoded_colum|\n",
      "+------+-----+----------+---------------+\n",
      "| Preco|   35|VideoGames| Value_Chumbado|\n",
      "| Peter|   31| Pizzaiolo| Value_Chumbado|\n",
      "|Felipe|   26| RichStuff| Value_Chumbado|\n",
      "+------+-----+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add a hardcoded column value\n",
    "#Criando uma coluna chumbada\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.withColumn(\"hardcoded_colum\", lit(\"Value_Chumbado\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|  Nome|Idade|     Hobby|\n",
      "+------+-----+----------+\n",
      "| Preco|   35|VideoGames|\n",
      "| Peter|   31| Pizzaiolo|\n",
      "|Felipe|   26| RichStuff|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The code above doesnot inplaced the original df\n",
    "#Veja que o código a cima não deu \"inplace\" no dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inplacing created column\n",
    "df = df.withColumn('Idade_x2',df['Idade']*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+--------+\n",
      "|  Nome|Idade|     Hobby|Idade_x2|\n",
      "+------+-----+----------+--------+\n",
      "| Preco|   35|VideoGames|      70|\n",
      "| Peter|   31| Pizzaiolo|      62|\n",
      "|Felipe|   26| RichStuff|      52|\n",
      "+------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming\n",
    "df = df.withColumnRenamed('Idade','Idade_renamed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------+--------+\n",
      "|  Nome|Idade_renamed|     Hobby|Idade_x2|\n",
      "+------+-------------+----------+--------+\n",
      "| Preco|           35|VideoGames|      70|\n",
      "| Peter|           31| Pizzaiolo|      62|\n",
      "|Felipe|           26| RichStuff|      52|\n",
      "+------+-------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "df = df.drop(\"Idade_x2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------+\n",
      "|  Nome|Idade_renamed|     Hobby|\n",
      "+------+-------------+----------+\n",
      "| Preco|           35|VideoGames|\n",
      "| Peter|           31| Pizzaiolo|\n",
      "|Felipe|           26| RichStuff|\n",
      "+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL inside Pyspark - Very usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "df.createOrReplaceTempView('DevDataX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.sql(\"\"\"SELECT * FROM DevDataX\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------+\n",
      "|  Nome|Idade_renamed|     Hobby|\n",
      "+------+-------------+----------+\n",
      "| Preco|           35|VideoGames|\n",
      "| Peter|           31| Pizzaiolo|\n",
      "|Felipe|           26| RichStuff|\n",
      "+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = spark.sql(\"\"\"SELECT * FROM DevDataX WHERE Idade_renamed=26\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------+\n",
      "|  Nome|Idade_renamed|    Hobby|\n",
      "+------+-------------+---------+\n",
      "|Felipe|           26|RichStuff|\n",
      "+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"appl_stock.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing DataFrames - Resumindo DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Em sql\n",
    "#A = spark.sql(\"\"\"SELECT * FROM df WHERE close < 500\"\"\").show()\n",
    "#Em spark dataframe\n",
    "df.filter(\"Close < 500\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      open|             close|\n",
      "+----------+------------------+\n",
      "|213.429998|        214.009998|\n",
      "|214.599998|        214.379993|\n",
      "|214.379993|        210.969995|\n",
      "|    211.75|            210.58|\n",
      "|210.299994|211.98000499999998|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 500\").select(['open','close']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   Volume|\n",
      "+---------+\n",
      "|123432400|\n",
      "|150476200|\n",
      "|138040000|\n",
      "|119282800|\n",
      "|111902700|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close'] < 500).select(\"Volume\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "|      Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Condições Multiplas (& : and, | : or)\n",
    "df.filter((df['Close'] < 200) & (df['Open'] > 200)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|      Date|              Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|2010-02-01|192.36999699999998|     196.0|191.29999899999999|194.729998|187469100|         25.229131|\n",
      "|2010-02-02|        195.909998|196.319994|193.37999299999998|195.859997|174585600|25.375532999999997|\n",
      "|2010-02-03|        195.169994|200.200003|        194.420004|199.229994|153832000|25.812148999999998|\n",
      "|2010-02-04|        196.730003|198.370001|        191.570005|192.050003|189413000|         24.881912|\n",
      "|2010-02-05|192.63000300000002|     196.0|        190.850002|195.460001|212576700|25.323710000000002|\n",
      "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Condições Multiplas (& : and, | : or)\n",
    "df.filter((df['Close'] < 200) & ~(df['Open'] > 200)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.filter(df['Low'] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220441900"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2010-01-04'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0].asDict()['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark Agregations -  Agregações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"sales_info.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   GOOG|            220.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "|     FB|            610.0|\n",
      "|   APPL|            370.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mean(), sum(), max(), min(), count()\n",
    "df.groupby(\"Company\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Coluna que vc quer agregar : tipo de agregacao\n",
    "df.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = df.groupby(\"Company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   GOOG|     340.0|\n",
      "|   MSFT|     600.0|\n",
      "|     FB|     870.0|\n",
      "|   APPL|     750.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   GOOG|     340.0|\n",
      "|   MSFT|     600.0|\n",
      "|     FB|     870.0|\n",
      "|   APPL|     750.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"Company\").agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct,avg,stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Average_Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales').alias('Average_Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|stddev_samp(Sales)|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_std = df.select(stddev('Sales').alias('std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               std|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Formatação de números para 2 casas decimais\n",
    "sales_std.select(format_number('std',2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Order by Ascending\n",
    "df.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Order by Descending\n",
    "df.orderBy(df['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing values in one single list value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Preco\",\"Barraco\"),\n",
    "        (\"Peter\",\"Sítio\"),\n",
    "        (\"Peter\",\"CasaPraia\"),\n",
    "        (\"Peter\",\"Apartment\"),\n",
    "        (\"Felipe\",\"Triplex\"),\n",
    "        (\"Felipe\",\"Fazenda\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Imóveis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  Nome|        Imóveis_List|\n",
      "+------+--------------------+\n",
      "|Felipe|  [Fazenda, Triplex]|\n",
      "| Peter|[Sítio, CasaPraia...|\n",
      "| Preco|           [Barraco]|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Nome').agg(functions.collect_set(\"Imóveis\").alias(\"Imóveis_List\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table - Columns to lines - Colunas para linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"sales_info.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+----+-------+-----+----+-----+----+----+-----+----+-------+\n",
      "|Company| Chris| Amy|Carl|Charlie|Frank|John|Linda|Mike| Sam|Sarah|Tina|Vanessa|\n",
      "+-------+------+----+----+-------+-----+----+-----+----+----+-----+----+-------+\n",
      "|   APPL| Chris|null|null|   null| null|John|Linda|Mike|null| null|null|   null|\n",
      "|     FB|  null|null|Carl|   null| null|null| null|null|null|Sarah|null|   null|\n",
      "|   GOOG|  null|null|null|Charlie|Frank|null| null|null| Sam| null|null|   null|\n",
      "|   MSFT|  null| Amy|null|   null| null|null| null|null|null| null|Tina|Vanessa|\n",
      "+-------+------+----+----+-------+-----+----+-----+----+----+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Company').pivot('Person').agg(first('Person')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt table - lines to columns - linhas para colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Preco\",\"Barraco\",\"JAN\"),\n",
    "        (\"Peter\",\"Sítio\",\"FEV\"),\n",
    "        (\"Peter\",\"CasaPraia\",\"JAN\"),\n",
    "        (\"Peter\",\"Apartment\",\"JAN\"),\n",
    "        (\"Felipe\",\"Triplex\",\"FEV\"),\n",
    "        (\"Felipe\",\"Fazenda\",\"FEV\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Imóveis\",\"Purchase_Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------+\n",
      "|  Nome|  Imóveis|Purchase_Month|\n",
      "+------+---------+--------------+\n",
      "| Preco|  Barraco|           JAN|\n",
      "| Peter|    Sítio|           FEV|\n",
      "| Peter|CasaPraia|           JAN|\n",
      "| Peter|Apartment|           JAN|\n",
      "|Felipe|  Triplex|           FEV|\n",
      "|Felipe|  Fazenda|           FEV|\n",
      "+------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode,array,struct,col\n",
    "def to_explode(df, by):      \n",
    "    # Filter dtypes and split into column names and type description\n",
    "    #Filtrando dtypes e separando em nome de coluna e tipo de coluna\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))     \n",
    "    # Spark SQL supports only homogeneous columns     \n",
    "    assert len(set(dtypes)) == 1, \"As colunas devem ter o mesmo tipo\"\n",
    "    # Create and explode an array of (column_name, column_value) structs \n",
    "    resultado = explode(array([struct(lit(c).alias(\"CATEGORIA\"),\n",
    "                                      col(c).alias(\"VALOR\")) for c in cols])).alias(\"resultado\")      \n",
    "    return df.select(by + [resultado]).select(by + \n",
    "            [\"resultado.CATEGORIA\", \"resultado.VALOR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=to_explode(df=df, by=['Nome','Purchase_Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------+---------+\n",
      "|  Nome|Purchase_Month|CATEGORIA|    VALOR|\n",
      "+------+--------------+---------+---------+\n",
      "| Preco|           JAN|  Imóveis|  Barraco|\n",
      "| Peter|           FEV|  Imóveis|    Sítio|\n",
      "| Peter|           JAN|  Imóveis|CasaPraia|\n",
      "| Peter|           JAN|  Imóveis|Apartment|\n",
      "|Felipe|           FEV|  Imóveis|  Triplex|\n",
      "|Felipe|           FEV|  Imóveis|  Fazenda|\n",
      "+------+--------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF Functions Register - Registrando funções do usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Preco\",35,\"VideoGames\"),\n",
    "        (\"Peter\",31,\"Pizzaiolo\"),\n",
    "        (\"Felipe\",26,\"RichStuff\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Idade\",\"Hobby\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|  Nome|Idade|     Hobby|\n",
      "+------+-----+----------+\n",
      "| Preco|   35|VideoGames|\n",
      "| Peter|   31| Pizzaiolo|\n",
      "|Felipe|   26| RichStuff|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Idade: long (nullable = true)\n",
      " |-- Hobby: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.raiz_quadrada(x)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "import math\n",
    "\n",
    "def raiz_quadrada(x):\n",
    "    return math.sqrt(x)\n",
    "\n",
    "spark.udf.register(\"raiz_quadrada\", raiz_quadrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('DevDataX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+------------------+\n",
      "|  Nome|Idade|     Hobby|        Raiz_Idade|\n",
      "+------+-----+----------+------------------+\n",
      "| Preco|   35|VideoGames| 5.916079783099616|\n",
      "| Peter|   31| Pizzaiolo|5.5677643628300215|\n",
      "|Felipe|   26| RichStuff|5.0990195135927845|\n",
      "+------+-----+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                    Nome,\n",
    "                    Idade,\n",
    "                    Hobby,\n",
    "                    raiz_quadrada(Idade) as Raiz_Idade\n",
    "                    \n",
    "                    FROM DevDataX\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/rodri/Desktop/Spark/spark-3.0.0-preview2-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark import SparkConf , SparkContext\n",
    "from pyspark.sql import SparkSession,functions\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "​\n",
    "config = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf=config)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "Creating Spark DataFrames - Criando DataFrames Spark\n",
    "data = [(\"Preco\",35,\"VideoGames\"),\n",
    "        (\"Peter\",31,\"Pizzaiolo\"),\n",
    "        (\"Felipe\",26,\"RichStuff\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Idade\",\"Hobby\"])\n",
    "#Show dataframe\n",
    "#Mostrar dataframe\n",
    "df.show()\n",
    "+------+-----+----------+\n",
    "|  Nome|Idade|     Hobby|\n",
    "+------+-----+----------+\n",
    "| Preco|   35|VideoGames|\n",
    "| Peter|   31| Pizzaiolo|\n",
    "|Felipe|   26| RichStuff|\n",
    "+------+-----+----------+\n",
    "\n",
    "df.printSchema()\n",
    "df.printSchema()\n",
    "root\n",
    " |-- Nome: string (nullable = true)\n",
    " |-- Idade: long (nullable = true)\n",
    " |-- Hobby: string (nullable = true)\n",
    "\n",
    "df.columns\n",
    "df.columns\n",
    "['Nome', 'Idade', 'Hobby']\n",
    "df.describe().show()\n",
    "+-------+------+------------------+----------+\n",
    "|summary|  Nome|             Idade|     Hobby|\n",
    "+-------+------+------------------+----------+\n",
    "|  count|     3|                 3|         3|\n",
    "|   mean|  null|30.666666666666668|      null|\n",
    "| stddev|  null| 4.509249752822894|      null|\n",
    "|    min|Felipe|                26| Pizzaiolo|\n",
    "|    max| Preco|                35|VideoGames|\n",
    "+-------+------+------------------+----------+\n",
    "\n",
    "Schema Defining - Definindo o Schema\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "#StructField -> Nome do campo, tipo do campo, True (pode conter nulos)\n",
    "data_schema = [\n",
    "               StructField('Nome',StringType(),True),\n",
    "               StructField('Idade',IntegerType(),True),\n",
    "               StructField('Hobby',StringType(),True)\n",
    "              ]\n",
    "final_struc = StructType(fields=data_schema)\n",
    "data = [(\"Preco\",35,\"VideoGames\"),\n",
    "        (\"Peter\",31,\"Pizzaiolo\"),\n",
    "        (\"Felipe\",26,\"RichStuff\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Idade\",\"Hobby\"],final_struc)\n",
    "df.show()\n",
    "+------+-----+----------+\n",
    "|  Nome|Idade|     Hobby|\n",
    "+------+-----+----------+\n",
    "| Preco|   35|VideoGames|\n",
    "| Peter|   31| Pizzaiolo|\n",
    "|Felipe|   26| RichStuff|\n",
    "+------+-----+----------+\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- Nome: string (nullable = true)\n",
    " |-- Idade: long (nullable = true)\n",
    " |-- Hobby: string (nullable = true)\n",
    "\n",
    "Spark Dataframe Stuff\n",
    "df.select('Idade').show()\n",
    "+-----+\n",
    "|Idade|\n",
    "+-----+\n",
    "|   35|\n",
    "|   31|\n",
    "|   26|\n",
    "+-----+\n",
    "\n",
    "df.head(3)\n",
    "[Row(Nome='Preco', Idade=35, Hobby='VideoGames'),\n",
    " Row(Nome='Peter', Idade=31, Hobby='Pizzaiolo'),\n",
    " Row(Nome='Felipe', Idade=26, Hobby='RichStuff')]\n",
    "df.head(3)[0]\n",
    "Row(Nome='Preco', Idade=35, Hobby='VideoGames')\n",
    "df.select(['Idade','Nome']).show()\n",
    "+-----+------+\n",
    "|Idade|  Nome|\n",
    "+-----+------+\n",
    "|   35| Preco|\n",
    "|   31| Peter|\n",
    "|   26|Felipe|\n",
    "+-----+------+\n",
    "\n",
    "#Add columns\n",
    "#Adicionar colunas\n",
    "df.withColumn('Idade_x2',df['Idade']*2).show()\n",
    "+------+-----+----------+--------+\n",
    "|  Nome|Idade|     Hobby|Idade_x2|\n",
    "+------+-----+----------+--------+\n",
    "| Preco|   35|VideoGames|      70|\n",
    "| Peter|   31| Pizzaiolo|      62|\n",
    "|Felipe|   26| RichStuff|      52|\n",
    "+------+-----+----------+--------+\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "#Add a hardcoded column value\n",
    "#Criando uma coluna chumbada\n",
    "from pyspark.sql.functions import lit\n",
    "​\n",
    "df.withColumn(\"hardcoded_colum\", lit(\"Value_Chumbado\")).show()\n",
    "+------+-----+----------+---------------+\n",
    "|  Nome|Idade|     Hobby|hardcoded_colum|\n",
    "+------+-----+----------+---------------+\n",
    "| Preco|   35|VideoGames| Value_Chumbado|\n",
    "| Peter|   31| Pizzaiolo| Value_Chumbado|\n",
    "|Felipe|   26| RichStuff| Value_Chumbado|\n",
    "+------+-----+----------+---------------+\n",
    "\n",
    "#The code above doesnot inplaced the original df\n",
    "#Veja que o código a cima não deu \"inplace\" no dataframe\n",
    "df.show()\n",
    "+------+-----+----------+\n",
    "|  Nome|Idade|     Hobby|\n",
    "+------+-----+----------+\n",
    "| Preco|   35|VideoGames|\n",
    "| Peter|   31| Pizzaiolo|\n",
    "|Felipe|   26| RichStuff|\n",
    "+------+-----+----------+\n",
    "\n",
    "#inplacing created column\n",
    "df = df.withColumn('Idade_x2',df['Idade']*2)\n",
    "df.show()\n",
    "+------+-----+----------+--------+\n",
    "|  Nome|Idade|     Hobby|Idade_x2|\n",
    "+------+-----+----------+--------+\n",
    "| Preco|   35|VideoGames|      70|\n",
    "| Peter|   31| Pizzaiolo|      62|\n",
    "|Felipe|   26| RichStuff|      52|\n",
    "+------+-----+----------+--------+\n",
    "\n",
    "#renaming\n",
    "df = df.withColumnRenamed('Idade','Idade_renamed')\n",
    "df.show()\n",
    "+------+-------------+----------+--------+\n",
    "|  Nome|Idade_renamed|     Hobby|Idade_x2|\n",
    "+------+-------------+----------+--------+\n",
    "| Preco|           35|VideoGames|      70|\n",
    "| Peter|           31| Pizzaiolo|      62|\n",
    "|Felipe|           26| RichStuff|      52|\n",
    "+------+-------------+----------+--------+\n",
    "\n",
    "#drop columns\n",
    "df = df.drop(\"Idade_x2\")\n",
    "df.show()\n",
    "+------+-------------+----------+\n",
    "|  Nome|Idade_renamed|     Hobby|\n",
    "+------+-------------+----------+\n",
    "| Preco|           35|VideoGames|\n",
    "| Peter|           31| Pizzaiolo|\n",
    "|Felipe|           26| RichStuff|\n",
    "+------+-------------+----------+\n",
    "\n",
    "SQL inside Pyspark - Very usefull\n",
    "#SQL\n",
    "df.createOrReplaceTempView('DevDataX')\n",
    "dataframe = spark.sql(\"\"\"SELECT * FROM DevDataX\"\"\")\n",
    "dataframe.show()\n",
    "+------+-------------+----------+\n",
    "|  Nome|Idade_renamed|     Hobby|\n",
    "+------+-------------+----------+\n",
    "| Preco|           35|VideoGames|\n",
    "| Peter|           31| Pizzaiolo|\n",
    "|Felipe|           26| RichStuff|\n",
    "+------+-------------+----------+\n",
    "\n",
    "new_dataframe = spark.sql(\"\"\"SELECT * FROM DevDataX WHERE Idade_renamed=26\"\"\")\n",
    "new_dataframe.show()\n",
    "+------+-------------+---------+\n",
    "|  Nome|Idade_renamed|    Hobby|\n",
    "+------+-------------+---------+\n",
    "|Felipe|           26|RichStuff|\n",
    "+------+-------------+---------+\n",
    "\n",
    "df = spark.read.csv(\"appl_stock.csv\",inferSchema=True,header=True)\n",
    "df.printSchema()\n",
    "root\n",
    " |-- Date: string (nullable = true)\n",
    " |-- Open: double (nullable = true)\n",
    " |-- High: double (nullable = true)\n",
    " |-- Low: double (nullable = true)\n",
    " |-- Close: double (nullable = true)\n",
    " |-- Volume: integer (nullable = true)\n",
    " |-- Adj Close: double (nullable = true)\n",
    "\n",
    "df.show(5)\n",
    "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
    "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
    "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
    "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
    "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
    "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
    "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
    "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
    "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "Summarizing DataFrames - Resumindo DataFrames\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "#Em sql\n",
    "#A = spark.sql(\"\"\"SELECT * FROM df WHERE close < 500\"\"\").show()\n",
    "#Em spark dataframe\n",
    "df.filter(\"Close < 500\").show(5)\n",
    "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
    "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
    "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
    "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
    "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
    "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
    "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
    "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
    "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "df.filter(\"Close < 500\").select(['open','close']).show(5)\n",
    "+----------+------------------+\n",
    "|      open|             close|\n",
    "+----------+------------------+\n",
    "|213.429998|        214.009998|\n",
    "|214.599998|        214.379993|\n",
    "|214.379993|        210.969995|\n",
    "|    211.75|            210.58|\n",
    "|210.299994|211.98000499999998|\n",
    "+----------+------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "df.filter(df['Close'] < 500).select(\"Volume\").show(5)\n",
    "+---------+\n",
    "|   Volume|\n",
    "+---------+\n",
    "|123432400|\n",
    "|150476200|\n",
    "|138040000|\n",
    "|119282800|\n",
    "|111902700|\n",
    "+---------+\n",
    "only showing top 5 rows\n",
    "\n",
    "#Condições Multiplas (& : and, | : or)\n",
    "df.filter((df['Close'] < 200) & (df['Open'] > 200)).show(5)\n",
    "+----------+------------------+----------+----------+----------+---------+------------------+\n",
    "|      Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
    "+----------+------------------+----------+----------+----------+---------+------------------+\n",
    "|2010-01-22|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
    "|2010-01-28|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
    "|2010-01-29|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
    "+----------+------------------+----------+----------+----------+---------+------------------+\n",
    "\n",
    "#Condições Multiplas (& : and, | : or)\n",
    "df.filter((df['Close'] < 200) & ~(df['Open'] > 200)).show(5)\n",
    "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
    "|      Date|              Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
    "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
    "|2010-02-01|192.36999699999998|     196.0|191.29999899999999|194.729998|187469100|         25.229131|\n",
    "|2010-02-02|        195.909998|196.319994|193.37999299999998|195.859997|174585600|25.375532999999997|\n",
    "|2010-02-03|        195.169994|200.200003|        194.420004|199.229994|153832000|25.812148999999998|\n",
    "|2010-02-04|        196.730003|198.370001|        191.570005|192.050003|189413000|         24.881912|\n",
    "|2010-02-05|192.63000300000002|     196.0|        190.850002|195.460001|212576700|25.323710000000002|\n",
    "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "result = df.filter(df['Low'] == 197.16).collect()\n",
    "row = result[0]\n",
    "row.asDict()['Volume']\n",
    "220441900\n",
    "df.collect()[0].asDict()['Date']\n",
    "'2010-01-04'\n",
    "Pyspark Agregations - Agregações\n",
    "df = spark.read.csv(\"sales_info.csv\",inferSchema=True,header=True)\n",
    "df.show()\n",
    "+-------+-------+-----+\n",
    "|Company| Person|Sales|\n",
    "+-------+-------+-----+\n",
    "|   GOOG|    Sam|200.0|\n",
    "|   GOOG|Charlie|120.0|\n",
    "|   GOOG|  Frank|340.0|\n",
    "|   MSFT|   Tina|600.0|\n",
    "|   MSFT|    Amy|124.0|\n",
    "|   MSFT|Vanessa|243.0|\n",
    "|     FB|   Carl|870.0|\n",
    "|     FB|  Sarah|350.0|\n",
    "|   APPL|   John|250.0|\n",
    "|   APPL|  Linda|130.0|\n",
    "|   APPL|   Mike|750.0|\n",
    "|   APPL|  Chris|350.0|\n",
    "+-------+-------+-----+\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- Company: string (nullable = true)\n",
    " |-- Person: string (nullable = true)\n",
    " |-- Sales: double (nullable = true)\n",
    "\n",
    "#mean(), sum(), max(), min(), count()\n",
    "df.groupby(\"Company\").mean().show()\n",
    "+-------+-----------------+\n",
    "|Company|       avg(Sales)|\n",
    "+-------+-----------------+\n",
    "|   GOOG|            220.0|\n",
    "|   MSFT|322.3333333333333|\n",
    "|     FB|            610.0|\n",
    "|   APPL|            370.0|\n",
    "+-------+-----------------+\n",
    "\n",
    "#Coluna que vc quer agregar : tipo de agregacao\n",
    "df.agg({'Sales':'sum'}).show()\n",
    "+----------+\n",
    "|sum(Sales)|\n",
    "+----------+\n",
    "|    4327.0|\n",
    "+----------+\n",
    "\n",
    "group_data = df.groupby(\"Company\")\n",
    "group_data.agg({'Sales':'max'}).show()\n",
    "+-------+----------+\n",
    "|Company|max(Sales)|\n",
    "+-------+----------+\n",
    "|   GOOG|     340.0|\n",
    "|   MSFT|     600.0|\n",
    "|     FB|     870.0|\n",
    "|   APPL|     750.0|\n",
    "+-------+----------+\n",
    "\n",
    "df.groupby(\"Company\").agg({'Sales':'max'}).show()\n",
    "+-------+----------+\n",
    "|Company|max(Sales)|\n",
    "+-------+----------+\n",
    "|   GOOG|     340.0|\n",
    "|   MSFT|     600.0|\n",
    "|     FB|     870.0|\n",
    "|   APPL|     750.0|\n",
    "+-------+----------+\n",
    "\n",
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "df.select(countDistinct('Sales')).show()\n",
    "+---------------------+\n",
    "|count(DISTINCT Sales)|\n",
    "+---------------------+\n",
    "|                   11|\n",
    "+---------------------+\n",
    "\n",
    "df.select(avg('Sales')).show()\n",
    "+-----------------+\n",
    "|       avg(Sales)|\n",
    "+-----------------+\n",
    "|360.5833333333333|\n",
    "+-----------------+\n",
    "\n",
    "df.select(avg('Sales')).show()\n",
    "+-----------------+\n",
    "|       avg(Sales)|\n",
    "+-----------------+\n",
    "|360.5833333333333|\n",
    "+-----------------+\n",
    "\n",
    "df.select(avg('Sales').alias('Average_Sales')).show()\n",
    "+-----------------+\n",
    "|    Average_Sales|\n",
    "+-----------------+\n",
    "|360.5833333333333|\n",
    "+-----------------+\n",
    "\n",
    "df.select(stddev('Sales')).show()\n",
    "+------------------+\n",
    "|stddev_samp(Sales)|\n",
    "+------------------+\n",
    "|250.08742410799007|\n",
    "+------------------+\n",
    "\n",
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.functions import format_number\n",
    "sales_std = df.select(stddev('Sales').alias('std'))\n",
    "sales_std.show()\n",
    "+------------------+\n",
    "|               std|\n",
    "+------------------+\n",
    "|250.08742410799007|\n",
    "+------------------+\n",
    "\n",
    "#Formatação de números para 2 casas decimais\n",
    "sales_std.select(format_number('std',2).alias('std')).show()\n",
    "+------+\n",
    "|   std|\n",
    "+------+\n",
    "|250.09|\n",
    "+------+\n",
    "\n",
    "df.show()\n",
    "+-------+-------+-----+\n",
    "|Company| Person|Sales|\n",
    "+-------+-------+-----+\n",
    "|   GOOG|    Sam|200.0|\n",
    "|   GOOG|Charlie|120.0|\n",
    "|   GOOG|  Frank|340.0|\n",
    "|   MSFT|   Tina|600.0|\n",
    "|   MSFT|    Amy|124.0|\n",
    "|   MSFT|Vanessa|243.0|\n",
    "|     FB|   Carl|870.0|\n",
    "|     FB|  Sarah|350.0|\n",
    "|   APPL|   John|250.0|\n",
    "|   APPL|  Linda|130.0|\n",
    "|   APPL|   Mike|750.0|\n",
    "|   APPL|  Chris|350.0|\n",
    "+-------+-------+-----+\n",
    "\n",
    "#Order by Ascending\n",
    "df.orderBy('Sales').show()\n",
    "+-------+-------+-----+\n",
    "|Company| Person|Sales|\n",
    "+-------+-------+-----+\n",
    "|   GOOG|Charlie|120.0|\n",
    "|   MSFT|    Amy|124.0|\n",
    "|   APPL|  Linda|130.0|\n",
    "|   GOOG|    Sam|200.0|\n",
    "|   MSFT|Vanessa|243.0|\n",
    "|   APPL|   John|250.0|\n",
    "|   GOOG|  Frank|340.0|\n",
    "|     FB|  Sarah|350.0|\n",
    "|   APPL|  Chris|350.0|\n",
    "|   MSFT|   Tina|600.0|\n",
    "|   APPL|   Mike|750.0|\n",
    "|     FB|   Carl|870.0|\n",
    "+-------+-------+-----+\n",
    "\n",
    "#Order by Descending\n",
    "df.orderBy(df['Sales'].desc()).show()\n",
    "+-------+-------+-----+\n",
    "|Company| Person|Sales|\n",
    "+-------+-------+-----+\n",
    "|     FB|   Carl|870.0|\n",
    "|   APPL|   Mike|750.0|\n",
    "|   MSFT|   Tina|600.0|\n",
    "|     FB|  Sarah|350.0|\n",
    "|   APPL|  Chris|350.0|\n",
    "|   GOOG|  Frank|340.0|\n",
    "|   APPL|   John|250.0|\n",
    "|   MSFT|Vanessa|243.0|\n",
    "|   GOOG|    Sam|200.0|\n",
    "|   APPL|  Linda|130.0|\n",
    "|   MSFT|    Amy|124.0|\n",
    "|   GOOG|Charlie|120.0|\n",
    "+-------+-------+-----+\n",
    "\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.functions import count\n",
    "df.show()\n",
    "+-------+-------+-----+\n",
    "|Company| Person|Sales|\n",
    "+-------+-------+-----+\n",
    "|   GOOG|    Sam|200.0|\n",
    "|   GOOG|Charlie|120.0|\n",
    "|   GOOG|  Frank|340.0|\n",
    "|   MSFT|   Tina|600.0|\n",
    "|   MSFT|    Amy|124.0|\n",
    "|   MSFT|Vanessa|243.0|\n",
    "|     FB|   Carl|870.0|\n",
    "|     FB|  Sarah|350.0|\n",
    "|   APPL|   John|250.0|\n",
    "|   APPL|  Linda|130.0|\n",
    "|   APPL|   Mike|750.0|\n",
    "|   APPL|  Chris|350.0|\n",
    "+-------+-------+-----+\n",
    "\n",
    "Summarizing values in one single list value\n",
    "data = [(\"Preco\",\"Barraco\"),\n",
    "        (\"Peter\",\"Sítio\"),\n",
    "        (\"Peter\",\"CasaPraia\"),\n",
    "        (\"Peter\",\"Apartment\"),\n",
    "        (\"Felipe\",\"Triplex\"),\n",
    "        (\"Felipe\",\"Fazenda\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Imóveis\"])\n",
    "df.groupby('Nome').agg(functions.collect_set(\"Imóveis\").alias(\"Imóveis_List\")).show()\n",
    "+------+--------------------+\n",
    "|  Nome|        Imóveis_List|\n",
    "+------+--------------------+\n",
    "|Felipe|  [Fazenda, Triplex]|\n",
    "| Peter|[Sítio, CasaPraia...|\n",
    "| Preco|           [Barraco]|\n",
    "+------+--------------------+\n",
    "\n",
    "Pivot table - Columns to lines - Colunas para linhas\n",
    "df = spark.read.csv(\"sales_info.csv\",inferSchema=True,header=True)\n",
    "df.show()\n",
    "+-------+-------+-----+\n",
    "|Company| Person|Sales|\n",
    "+-------+-------+-----+\n",
    "|   GOOG|    Sam|200.0|\n",
    "|   GOOG|Charlie|120.0|\n",
    "|   GOOG|  Frank|340.0|\n",
    "|   MSFT|   Tina|600.0|\n",
    "|   MSFT|    Amy|124.0|\n",
    "|   MSFT|Vanessa|243.0|\n",
    "|     FB|   Carl|870.0|\n",
    "|     FB|  Sarah|350.0|\n",
    "|   APPL|   John|250.0|\n",
    "|   APPL|  Linda|130.0|\n",
    "|   APPL|   Mike|750.0|\n",
    "|   APPL|  Chris|350.0|\n",
    "+-------+-------+-----+\n",
    "\n",
    "df.groupBy('Company').pivot('Person').agg(first('Person')).show()\n",
    "+-------+------+----+----+-------+-----+----+-----+----+----+-----+----+-------+\n",
    "|Company| Chris| Amy|Carl|Charlie|Frank|John|Linda|Mike| Sam|Sarah|Tina|Vanessa|\n",
    "+-------+------+----+----+-------+-----+----+-----+----+----+-----+----+-------+\n",
    "|   APPL| Chris|null|null|   null| null|John|Linda|Mike|null| null|null|   null|\n",
    "|     FB|  null|null|Carl|   null| null|null| null|null|null|Sarah|null|   null|\n",
    "|   GOOG|  null|null|null|Charlie|Frank|null| null|null| Sam| null|null|   null|\n",
    "|   MSFT|  null| Amy|null|   null| null|null| null|null|null| null|Tina|Vanessa|\n",
    "+-------+------+----+----+-------+-----+----+-----+----+----+-----+----+-------+\n",
    "\n",
    "Melt table - lines to columns - linhas para colunas\n",
    "data = [(\"Preco\",\"Barraco\",\"JAN\"),\n",
    "        (\"Peter\",\"Sítio\",\"FEV\"),\n",
    "        (\"Peter\",\"CasaPraia\",\"JAN\"),\n",
    "        (\"Peter\",\"Apartment\",\"JAN\"),\n",
    "        (\"Felipe\",\"Triplex\",\"FEV\"),\n",
    "        (\"Felipe\",\"Fazenda\",\"FEV\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Imóveis\",\"Purchase_Month\"])\n",
    "df.show()\n",
    "+------+---------+--------------+\n",
    "|  Nome|  Imóveis|Purchase_Month|\n",
    "+------+---------+--------------+\n",
    "| Preco|  Barraco|           JAN|\n",
    "| Peter|    Sítio|           FEV|\n",
    "| Peter|CasaPraia|           JAN|\n",
    "| Peter|Apartment|           JAN|\n",
    "|Felipe|  Triplex|           FEV|\n",
    "|Felipe|  Fazenda|           FEV|\n",
    "+------+---------+--------------+\n",
    "\n",
    "from pyspark.sql.functions import split, explode,array,struct,col\n",
    "def to_explode(df, by):      \n",
    "    # Filter dtypes and split into column names and type description\n",
    "    #Filtrando dtypes e separando em nome de coluna e tipo de coluna\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))     \n",
    "    # Spark SQL supports only homogeneous columns     \n",
    "    assert len(set(dtypes)) == 1, \"As colunas devem ter o mesmo tipo\"\n",
    "    # Create and explode an array of (column_name, column_value) structs \n",
    "    resultado = explode(array([struct(lit(c).alias(\"CATEGORIA\"), col(c).alias(\"VALOR\")) for c in cols])).alias(\"resultado\")      \n",
    "    return df.select(by + [resultado]).select(by + [\"resultado.CATEGORIA\", \"resultado.VALOR\"])\n",
    "df2=to_explode(df=df, by=['Nome','Purchase_Month'])\n",
    "df2.show()\n",
    "+------+--------------+---------+---------+\n",
    "|  Nome|Purchase_Month|CATEGORIA|    VALOR|\n",
    "+------+--------------+---------+---------+\n",
    "| Preco|           JAN|  Imóveis|  Barraco|\n",
    "| Peter|           FEV|  Imóveis|    Sítio|\n",
    "| Peter|           JAN|  Imóveis|CasaPraia|\n",
    "| Peter|           JAN|  Imóveis|Apartment|\n",
    "|Felipe|           FEV|  Imóveis|  Triplex|\n",
    "|Felipe|           FEV|  Imóveis|  Fazenda|\n",
    "+------+--------------+---------+---------+\n",
    "\n",
    "UDF Functions Register - Registrando funções do usuário\n",
    "data = [(\"Preco\",35,\"VideoGames\"),\n",
    "        (\"Peter\",31,\"Pizzaiolo\"),\n",
    "        (\"Felipe\",26,\"RichStuff\")]\n",
    "df = spark.createDataFrame(data, [\"Nome\",\"Idade\",\"Hobby\"])\n",
    "df.show()\n",
    "+------+-----+----------+\n",
    "|  Nome|Idade|     Hobby|\n",
    "+------+-----+----------+\n",
    "| Preco|   35|VideoGames|\n",
    "| Peter|   31| Pizzaiolo|\n",
    "|Felipe|   26| RichStuff|\n",
    "+------+-----+----------+\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- Nome: string (nullable = true)\n",
    " |-- Idade: long (nullable = true)\n",
    " |-- Hobby: string (nullable = true)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "​\n",
    "import math\n",
    "​\n",
    "def raiz_quadrada(x):\n",
    "    return math.sqrt(x)\n",
    "​\n",
    "spark.udf.register(\"raiz_quadrada\", raiz_quadrada)\n",
    "<function __main__.raiz_quadrada(x)>\n",
    "df.createOrReplaceTempView('DevDataX')\n",
    "df2 = spark.sql(\"\"\"\n",
    "\n",
    "df2 = spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                    Nome,\n",
    "                    Idade,\n",
    "                    Hobby,\n",
    "                    raiz_quadrada(Idade) as Raiz_Idade\n",
    "                    \n",
    "                    FROM DevDataX\n",
    "\"\"\").show()\n",
    "+------+-----+----------+------------------+\n",
    "|  Nome|Idade|     Hobby|        Raiz_Idade|\n",
    "+------+-----+----------+------------------+\n",
    "| Preco|   35|VideoGames| 5.916079783099616|\n",
    "| Peter|   31| Pizzaiolo|5.5677643628300215|\n",
    "|Felipe|   26| RichStuff|5.0990195135927845|\n",
    "+------+-----+----------+------------------+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
